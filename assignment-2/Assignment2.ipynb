{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "#### 02806 - Social data analysis and visualization \n",
    "\n",
    "_____\n",
    "_____\n",
    "_____\n",
    "_____\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Questions to text and lectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 1.1***\n",
    "\n",
    "*What is the Oxford English Dictionary's defintion of a narrative?*\n",
    "\n",
    "***Answer:*** \n",
    "\n",
    "*“An account of a series of events, facts, etc., given in order and with the establishing of connections between them.”*\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 1.2***\n",
    "\n",
    "*What is your favorite visualization among the examples in section 3? Explain why in a few words.*\n",
    "\n",
    "***Answer:***\n",
    "\n",
    "*Our favorite is 3.2 - 'Budget Forecasts'. The idea of single-frame interactivity is excellent, since it compacts the visualisation, while still given a great deal of information. It also has a nice tacit tutorial, allowing users to participate, as if they are a part of exploring the visualisation. The added mouse-over (tooltip) details augments this compact graph with enough information - when you need it.*\n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 1.3***\n",
    "\n",
    "*What's the point of Figure 7?*\n",
    "\n",
    "***Answer:*** *The point with the figure is that a lot of different genres of presentation all have som ordering of the data element, and an interactive element of the data visualisation. However, most of them actually fail in (3) giving the user a tutorial on how to use it, or (4) don't use enough messaging to clearly narrate the story*\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 1.4***\n",
    "\n",
    "*Use Figure 7 to find the most common design choice within each category for the Visual narrative and Narrative structure (the categories within visual narrative are 'visual structuring', 'highlighting', etc).*\n",
    "\n",
    "***Answer:***\n",
    "\n",
    "*The most common design choices in each category is:*\n",
    "\n",
    "- **Visual narrative**\n",
    "    - *Visual structuring*\n",
    "        - *Consistent Visual Platform*\n",
    "    - *Highlighting*\n",
    "        - *Feature distinction*\n",
    "    - *Transition guidance*\n",
    "        - *Object continuity*\n",
    "- **Narrative structure***\n",
    "    - *Ordering*\n",
    "        - *User directed path*\n",
    "    - *Interactivity*\n",
    "        - *Filtering / Selection / Search*\n",
    "    - *Messaging*\n",
    "        - *Captions / Headlines*\n",
    "        \n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 1.5*** \n",
    "\n",
    "*Check out Figure 8 and section 4.3. What is your favorite genre of narrative visualization? Why? What is your least favorite genre? Why?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Answer:***\n",
    "\n",
    "*Our favorite genre is Partitioned Poster. A poster can carry a wealth of information, including charts and legends, and can partition the information and present it intuitively. The vivid illustrations are full of fun, and the process of getting the details from the above is very inspiring. My least favorite is Flow Chart for it is boring and unintuitive. Furthermore, flow chart can't reveal changes in time. It will inculcate readers too strongly in the logic of the matter which may fail to combine with some details.*\n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 1.6***\n",
    "\n",
    "*What are the three key elements to keep in mind when you design an explanatory visualization?*\n",
    "\n",
    "***Answer:*** *There are 3 key elements:*\n",
    "\n",
    "- *Start with the question (What is the question that you want to answer and communicate?)*\n",
    "- *Allow exploration (Let the users investigate the data. Therefore something like D3, or even Tableau can be used)*\n",
    "- *Know you readers (Make sure that you are not dumbing something down for instance to peers. Or making it overly complicated when it needs to be cross-disciplinary)*\n",
    "____\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 1.7***\n",
    "\n",
    "*In the video I talk about (1) overview first, (2) zoom and filter, (3) details on demand.*\n",
    "- *Go online and find a visualization that follows these principles (don't use one from the video).*\n",
    "- *Explain how it does achieves (1)-(3). It might be useful to use screenshots to illustrate your explanation.*\n",
    "\n",
    "***Answer:***\n",
    "- *Online we found [this link](http://manpopex.us/)*\n",
    "- *(1) First you get a good overview of dynamic population of Manhattan. (2) Then you can zoom in and filter the data that you need, for instance, we can select one district, instead of the 12 districts highlighted. (3) Then we can specifically select data that happens on each day, to see how much it changes over the day*\n",
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 1.8***\n",
    "\n",
    "*Explain in your own words: How is explanatory data analysis different from exploratory data analysis?*\n",
    "\n",
    "***Answer:*** *Exploratory data analysis is where you are looking for correlations and interesting stories that the data has to tell you. This is opposed to the explanatory data analysis, where you've found the interesting story that the data has to show, and you've spent time on visualising that story, so that others may see it as well.*\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Random forest and weather\n",
    "\n",
    "The aim here is to recreate the work you did in Part 1-3 of the Week 7 lecture. I've phrased things differently relative to the exercise to make the purpose more clear.\n",
    "\n",
    "This exercise is divided into two parts:\n",
    "\n",
    "- **Part 2A** - Random forest binary classification\n",
    "- **Part 2B** - Info from weather features\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2A: Random forest binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using the and instructions and material from Week 7, build a *random forest* classifier to distinguish between two types (you choose) of crime using on spatio-temporal (where/when) features of data describing the two crimes. When you're done, you should be able to give the classifier a place and a time, and it should tell you which of the two  types of crime happened there.\n",
    "  - Explain about your choices for training/test data, features, and encoding. (You decide how to present your results, but here are some example topics to consider: Did you balance the training data? What are the pros/cons of balancing? Do you think your model is overfitting? Did you choose to do cross-validation? Which specific features did you end up using? Why? Which features (if any) did you one-hot encode? Why ... or why not?))\n",
    "  - Report accuracy. Discuss the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***We start off by importing pandas, numpy, and the policing dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_all = pd.read_csv('../rawdata/Police_Department_Incident_Reports__Historical_2003_to_May_2018.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***We do some filtering of the data, so that we only have the `CATEGORY`, `DAYOFWEEK`, `TIME`, `PDDISTRICT`, AND `MONTH`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Time</th>\n",
       "      <th>PdDistrict</th>\n",
       "      <th>Month</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-08-20 04:00:00+07:00</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-06 17:00:00+07:00</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-07-31 00:00:00+07:00</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-05-23 21:00:00+07:00</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-02-28 22:00:00+07:00</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-05-18 03:00:00+07:00</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-06-09 00:00:00+07:00</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-05-19 17:00:00+07:00</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-06-23 00:00:00+07:00</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-08-27 22:00:00+07:00</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Category  DayOfWeek  Time  PdDistrict  Month\n",
       "Date                                                                   \n",
       "2005-08-20 04:00:00+07:00         0          5     3           4      8\n",
       "2009-05-06 17:00:00+07:00         0          2    16           9      5\n",
       "2011-07-31 00:00:00+07:00         0          6     0           3      7\n",
       "2006-05-23 21:00:00+07:00         0          1    20           7      5\n",
       "2012-02-28 22:00:00+07:00         0          1    22           9      2\n",
       "2011-05-18 03:00:00+07:00         0          2     3           4      5\n",
       "2011-06-09 00:00:00+07:00         0          3     0           3      6\n",
       "2004-05-19 17:00:00+07:00         0          2    17           3      5\n",
       "2011-06-23 00:00:00+07:00         0          3     0           3      6\n",
       "2008-08-27 22:00:00+07:00         0          2    22           3      8"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We chose the columns of interest\n",
    "df_all = df_all.loc[:, [\"Category\",\"DayOfWeek\",\"Date\",\"Time\",\"PdDistrict\"]]\n",
    "\n",
    "# We create a dataset for prostitution\n",
    "df_pro = df_all[df_all['Category'] == 'PROSTITUTION']\n",
    "# We create a dataset for vandalism\n",
    "df_van = df_all[df_all['Category'] == 'VANDALISM']\n",
    "# We sample 15000 data entries from each dataset to create a balanced dataset\n",
    "df_pro = df_pro.sample(15000)\n",
    "df_van = df_van.sample(15000)\n",
    "# We concat the datasets\n",
    "df = pd.concat([df_pro,df_van])\n",
    "\n",
    "# We convert the hour to just be the hour, without the minutes (i.e 22:23 = 22:00)\n",
    "# and also convert it to the appropriate timezone to match the weather data later\n",
    "df[\"Date\"] = df.apply(lambda x: pd.to_datetime(x.Date + \" \" + x.Time).round(\"H\").tz_localize(\"ETC/GMT-7\"), axis = 1)\n",
    "\n",
    "# We convert the day of the week into an integer\n",
    "df['DayOfWeek'] = df['DayOfWeek'].map({'Monday':0,'Tuesday':1,'Wednesday':2,'Thursday':3,'Friday':4,'Saturday':5,'Sunday':6})\n",
    "\n",
    "# We convert the district into an integer \n",
    "df['PdDistrict'] = df['PdDistrict'].map({'BAYVIEW':0,'CENTRAL':1,'INGLESIDE':2,'MISSION':3,'NORTHERN':4,'PARK':5,'RICHMOND':6,'SOUTHERN':7,'TARAVAL':8,'TENDERLOIN':9})\n",
    "\n",
    "# We extract the month from the time data\n",
    "df['Month'] = pd.to_datetime(df['Date']).dt.month\n",
    "\n",
    "# We extract the hour from the time data\n",
    "df['Time'] = pd.to_datetime(df['Time']).dt.hour\n",
    "\n",
    "# We make the categorical data for prostitution and vandalism into integers\n",
    "df['Category'] = df['Category'].map({'PROSTITUTION':0,'VANDALISM':1})\n",
    "\n",
    "# We set the index to the datetime\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# And display the first 10 incidents in the dataframe\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***A training and test dataset is chosen with the use of the sklearn.model_selection package***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We choose our features\n",
    "X = df.iloc[:, 1:5].values\n",
    "# We choose targets\n",
    "y = df.iloc[:, 0].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We split it in training and test data with our features and target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Now a fit is made on the dataset with a random forest classifier***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialise the randomforestclassifier as classifier, with 20 estimators\n",
    "classifier = RandomForestClassifier(n_estimators=20, random_state=20)\n",
    "# We fit the classifier on our training set with features and target\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# We make a prediction on the test set \n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# We a prediction on the training set\n",
    "y_train_pred = classifier.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Now the training and test set accuracy can be reported as follows:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set accuracy:\n",
      "0.8895925925925926\n",
      "test set accuracy:\n",
      "0.807\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "#print(confusion_matrix(y_test,y_pred))\n",
    "#print(classification_report(y_test,y_pred))\n",
    "print('training set accuracy:')\n",
    "print(accuracy_score(y_train, y_train_pred))\n",
    "print('test set accuracy:')\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*From the incident reports dataset, the two categories `PROSTITUTION` and `VANDALISM` was chosen. The training and test set was divided at a ratio of 9:1 with no validation set. In order to insure a balanced dataset, 15000 data points were sampled from both datasets. Using a balanced dataset should greatly increase the model prediction accuracy in the future, as it is not skewed by an uneven amount of data. For instance, if we had 15000 datapoints with the `PROSTITUTION` category and 1000 points of the `VANDALISM` category, then the model is much more likely to predict `PROSTITUTION` over `VANDLISM`*\n",
    "\n",
    "In the above model, it is clear that the training set prediction accuracy is 89% compared to the test set prediction accuracy of 82%. This is of course not a massive descrepancy, but it should be taken into consideration, if the model prediction accuracy should be improved - ideally it will give the same accuracy for the training and test set. The small difference in prediction accuracy may be accredited to the fact that the Random Forest has been used over just a Decision Tree - this is a good way of eliminating the risk of overfitting on datasets, since it will generate several decision trees and then use the model that overfits the least but with the higest prediction accuracy possible. \n",
    "\n",
    "In this case, one-hot encoding was not used - categorical data was transformed into integers to allow for easier interpretation of the classifier.\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2B: Info from weather features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now add features from weather data to your random forest. \n",
    "  - Report accuracy. \n",
    "  - Discuss how the model performance changes relative to the version with no weather data.\n",
    "  - Discuss what you have learned about crime from including weather data in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The weather file is imported and formatted as necessary:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot convert tz-naive Timestamp, use tz_localize to localize",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/home/msei/anaconda3/envs/SCDS2021/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mconverter\u001b[0;34m(*date_cols)\u001b[0m\n\u001b[1;32m   3020\u001b[0m                 result = tools.to_datetime(\n\u001b[0;32m-> 3021\u001b[0;31m                     date_parser(*date_cols), errors='ignore')\n\u001b[0m\u001b[1;32m   3022\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-6a510e9fc5ef>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m                       \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                       date_parser=lambda x: pd.to_datetime(x).tz_convert(None).tz_localize(\"Etc/GMT+3\").tz_convert(\"Etc/GMT-7\"))\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/msei/anaconda3/envs/SCDS2021/lib/python3.5/site-packages/pandas/core/indexes/datetimes.py\u001b[0m in \u001b[0;36mtz_convert\u001b[0;34m(self, tz)\u001b[0m\n\u001b[1;32m   2294\u001b[0m             \u001b[0;31m# tz naive, use tz_localize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2295\u001b[0;31m             raise TypeError('Cannot convert tz-naive timestamps, use '\n\u001b[0m\u001b[1;32m   2296\u001b[0m                             'tz_localize to localize')\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert tz-naive timestamps, use tz_localize to localize",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/home/msei/anaconda3/envs/SCDS2021/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mconverter\u001b[0;34m(*date_cols)\u001b[0m\n\u001b[1;32m   3029\u001b[0m                                                 \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdate_parser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3030\u001b[0;31m                                                 dayfirst=dayfirst),\n\u001b[0m\u001b[1;32m   3031\u001b[0m                         errors='ignore')\n",
      "\u001b[0;32mpandas/_libs/tslibs/parsing.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.parsing.try_parse_dates\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslibs/parsing.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.parsing.try_parse_dates\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-6a510e9fc5ef>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m                       \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                       date_parser=lambda x: pd.to_datetime(x).tz_convert(None).tz_localize(\"Etc/GMT+3\").tz_convert(\"Etc/GMT-7\"))\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslibs/timestamps.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.timestamps.Timestamp.tz_convert\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert tz-naive Timestamp, use tz_localize to localize",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-6a510e9fc5ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m weather = pd.read_csv(\"weather.csv\", \n\u001b[1;32m      3\u001b[0m                       \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                       date_parser=lambda x: pd.to_datetime(x).tz_convert(None).tz_localize(\"Etc/GMT+3\").tz_convert(\"Etc/GMT-7\"))\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# A `Date` column is created\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/msei/anaconda3/envs/SCDS2021/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/msei/anaconda3/envs/SCDS2021/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/msei/anaconda3/envs/SCDS2021/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1034\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/msei/anaconda3/envs/SCDS2021/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1919\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1921\u001b[0;31m             \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_date_conversions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1922\u001b[0m             \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malldata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/msei/anaconda3/envs/SCDS2021/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_do_date_conversions\u001b[0;34m(self, names, data)\u001b[0m\n\u001b[1;32m   1673\u001b[0m             data, names = _process_date_conversion(\n\u001b[1;32m   1674\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_date_conv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1675\u001b[0;31m                 self.index_names, names, keep_date_col=self.keep_date_col)\n\u001b[0m\u001b[1;32m   1676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1677\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/msei/anaconda3/envs/SCDS2021/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_process_date_conversion\u001b[0;34m(data_dict, converter, parse_spec, index_col, index_names, columns, keep_date_col)\u001b[0m\n\u001b[1;32m   3064\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_isindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m                 \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolspec\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolspec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3067\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m                 new_name, col, old_names = _try_convert_dates(\n",
      "\u001b[0;32m/home/msei/anaconda3/envs/SCDS2021/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mconverter\u001b[0;34m(*date_cols)\u001b[0m\n\u001b[1;32m   3031\u001b[0m                         errors='ignore')\n\u001b[1;32m   3032\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3033\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mgeneric_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_parser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdate_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3035\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/msei/anaconda3/envs/SCDS2021/lib/python3.5/site-packages/pandas/io/date_converters.py\u001b[0m in \u001b[0;36mgeneric_parser\u001b[0;34m(parse_func, *cols)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-6a510e9fc5ef>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      2\u001b[0m weather = pd.read_csv(\"weather.csv\", \n\u001b[1;32m      3\u001b[0m                       \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                       date_parser=lambda x: pd.to_datetime(x).tz_convert(None).tz_localize(\"Etc/GMT+3\").tz_convert(\"Etc/GMT-7\"))\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# A `Date` column is created\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslibs/timestamps.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.timestamps.Timestamp.tz_convert\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert tz-naive Timestamp, use tz_localize to localize"
     ]
    }
   ],
   "source": [
    "# The weather file is imported\n",
    "weather = pd.read_csv(\"weather.csv\", \n",
    "                      parse_dates=[\"date\"],\n",
    "                      date_parser=lambda x: pd.to_datetime(x).tz_convert(None).tz_localize(\"Etc/GMT+3\").tz_convert(\"Etc/GMT-7\"))\n",
    "\n",
    "# A `Date` column is created\n",
    "weather['Date'] = weather['date']\n",
    "\n",
    "# The `date` column is removed\n",
    "weather = weather.drop('date', 1)\n",
    "\n",
    "# The Date is converted to the index\n",
    "weather.set_index('Date', inplace=True)\n",
    "\n",
    "# The weather data is made categorical\n",
    "weather['weather'] = pd.Categorical(weather['weather'], categories=weather['weather'].unique()).codes\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The weather data is merged with the incidents dataframe from the previous exercise\n",
    "df_merge = pd.merge(df, weather, how='inner', left_index=True, right_index=True).dropna()\n",
    "df_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The features are chosen as X and the target is chosen as y***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_merge.iloc[:, 1:11].values\n",
    "y = df_merge.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The random forest classifier is initialised with 20 trees***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=20, random_state=20)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The reported prediction accuracy can now be calculated as follows:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Above, it was seen that the overall prediction accuracy of the random forest classifier on the test set was 92%. This is means that the weather data has improved the model accuracy with 10% from the previous exercise. Therefore, it goes to show that weather plays an important role in predicting some of the crime patterns that can be seen in San Francisco. Furthermore, from researching this topic online, [it seems that several news journals, like this one](https://www.zmescience.com/science/weather-crime-connection-04234/) report this phenomenon. This information is crucial, in helping law enforcement in dealing with a temperature rise and the following violent acts that follow.*\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data visualization\n",
    "\n",
    "***Exercise 3***\n",
    "\n",
    "- Create the Bokeh visualization from Part 2 of the Week 8 Lecture, displayed in a beautiful `.gif` below. \n",
    "- Provide nice comments for your code. Don't just use the `# inline comments`, but the full Notebook markdown capabilities and explain what you're doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Movie](./bokeh.gif \"movie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Start off by importing the policing dataframe and parse the date and time as a datetime***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../rawdata/Police_Department_Incident_Reports__Historical_2003_to_May_2018.csv', parse_dates=[['Date', 'Time']])\n",
    "df = df[(df['Date_Time'].dt.year >= 2010) & (df['Date_Time'].dt.year <= 2018)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In this case, we want to use the focuscrimes, provided earlier, to make the dataset more managable. These will be the features that we can later select to include or disclude from our bokeh visualisation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focuscrimes = ['WEAPON LAWS', 'PROSTITUTION', 'DRIVING UNDER THE INFLUENCE', 'ROBBERY', 'BURGLARY', 'ASSAULT', 'DRUNKENNESS', 'DRUG/NARCOTIC', 'TRESPASS', 'LARCENY/THEFT', 'VANDALISM', 'VEHICLE THEFT', 'STOLEN PROPERTY', 'DISORDERLY CONDUCT']\n",
    "\n",
    "df_norm = pd.DataFrame(np.zeros((24, len(focuscrimes))),columns=focuscrimes)\n",
    "df_norm['Hour'] = [x for x in range(1, 25)]\n",
    "\n",
    "# we iterate over the focuscrimes to add them to the bokeh visualisation\n",
    "for i, crime_type in enumerate(focuscrimes):\n",
    "    df_CT = df[df['Category'] == crime_type]\n",
    "    countByHour_CT = df_CT.groupby(df['Date_Time'].dt.hour).count()[\"PdId\"]\n",
    "    df_norm[crime_type] = [float(i)/sum(countByHour_CT.values) for i in countByHour_CT.values]\n",
    "df_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We import the bokeh.io library and check that bokeh is loaded*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook, show\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We import the plotting and models methods to use for later plotting*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure\n",
    "from bokeh.models import ColumnDataSource, FactorRange, Legend\n",
    "\n",
    "source = ColumnDataSource(df_norm)\n",
    "p = figure(plot_width=1000, plot_height=400, x_range = FactorRange(factors=list(map(str,df_norm['Hour'].values.tolist()))), title=\"Crimes By Hour\", x_axis_label='Hour of The Day', y_axis_label='Relative Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*X axis is 24-hour which is set by 'FactorRange', a Range of values for a categorical dimesion. Other parameters are configured to set up a Bokeh figure.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.palettes import Category20\n",
    "\n",
    "bar = {}\n",
    "items = []\n",
    "for indx,(i,color) in enumerate(zip(focuscrimes, Category20[14])):\n",
    "    bar[i] = p.vbar(x='Hour', top=i, source=source, alpha=1, color=color, width=2, muted_alpha=0.08, muted_color=color, muted=True)\n",
    "    items.append((i, [bar[i]]))\n",
    "legend = Legend(items=items, location=(0,-30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To construct a bar plot, both x-axis and y-axis is selected from the Dataframe. 'alpha' decides the color depth. 'muted_color' and 'muted_alpha' are set for the interactive mode is 'Muted'.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.add_layout(legend, \"right\")\n",
    "p.legend.click_policy=\"mute\"\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Legend is configured separately and can be clicked to mute specific bar.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
