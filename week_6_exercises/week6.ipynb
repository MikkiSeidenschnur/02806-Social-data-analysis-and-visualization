{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "norwegian-baptist",
   "metadata": {},
   "source": [
    "# Week 6 - Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-polyester",
   "metadata": {},
   "source": [
    "## Part 1: Lightning intro to machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-valley",
   "metadata": {},
   "source": [
    "***Exercise 1.1***\n",
    "\n",
    "*What do we mean by a 'feature' in a machine learning model?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-creek",
   "metadata": {},
   "source": [
    "***Answer:*** *A feature represents a characteristic in machine learning, such as the Sex of a person. Another way to look at 'features' is to compare it to a face of a person - a facial feature could be they have blue eyes. Therefore, you could say that it is an individual measureable characteristic*\n",
    "___________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-marker",
   "metadata": {},
   "source": [
    "***Exercise 1.2***\n",
    "\n",
    "*What is the problem with overfitting?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-gateway",
   "metadata": {},
   "source": [
    "***Answer:*** *Overfitting really allows for the training set to be extremely well predicted, whilst it fails to generalize for that type of data. In other words, it means that the model will pick up noise and fluctutations, that do not represent the trend of the data.*\n",
    "___________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-penetration",
   "metadata": {},
   "source": [
    "***Exercise 1.3***\n",
    "\n",
    "*Explain the connection between the bias-variance trade-off and overfitting/underfitting*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-border",
   "metadata": {},
   "source": [
    "***Answer:*** *I found this very interesting [infographic](https://elitedatascience.com/bias-variance-tradeoff).*\n",
    "\n",
    "- *A model that has high bias and low variance are generally consistent, but inaccurate*\n",
    "- *A model with high variance and low bias are more accurate on average, but inconsistent*\n",
    "\n",
    "***What is the tradeoff then?*** *If you have a high bias, then you will have a low variance. The same goes for having a high variance, then you will have a low variance. Therefore the training/test sets should be varied, to minimize bias, but also try to tune the model with impactful parameters. Like if you were doing some textual ML algorithm, you could sometimes exclude outliers, as they tend not to add prediction accuracy.*\n",
    "___________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-figure",
   "metadata": {},
   "source": [
    "***Exercise 1.4*** \n",
    "\n",
    "*The Luke is for leukemia on page 145 in the reading is a great example of why accuracy is not a good measure in very unbalanced problems. You know about the incidents dataset we've been working with. Try to come up with a similar example based on the data we've been working with today.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-native",
   "metadata": {},
   "source": [
    "***Answer:*** **\n",
    "___________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-marriage",
   "metadata": {},
   "source": [
    "## Part 2: Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-position",
   "metadata": {},
   "source": [
    "__________\n",
    "***Run through the first three sections of this [tutorial](https://scikit-learn.org/stable/tutorial/basic/tutorial.html)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-contest",
   "metadata": {},
   "source": [
    "**Loading an example dataset**\n",
    "\n",
    "*We start of by importing the datasets we need for the tutorial*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-glass",
   "metadata": {},
   "source": [
    "The data that gives access to the features is stored in the digits.data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-fifth",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-alloy",
   "metadata": {},
   "source": [
    "And the target can be accessed (supervised machine learning) in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(digits.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-filling",
   "metadata": {},
   "source": [
    "**Loading an example dataset**\n",
    "\n",
    "*In scikit learn we can make an estimator with the use of:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-retirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma=0.001, C=100.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-condition",
   "metadata": {},
   "source": [
    "In order to let the classifier learn from the model, we must now pass the fit of the dataset into it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(digits.data[:-1], digits.target[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-halloween",
   "metadata": {},
   "source": [
    "Now, the classifier has learned from the fit that we did, we can therefore predict what kind of letter or number is written:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-adolescent",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(digits.data[-1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-commerce",
   "metadata": {},
   "source": [
    "*From the image, it is hard to make out if it is actually the number 8 that has been written, due to poor resolution.*\n",
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-lincoln",
   "metadata": {},
   "source": [
    "***Exercise 2.1***\n",
    "\n",
    "*Did you read the text?*\n",
    "\n",
    "- ***Describe in your own words how data is organized in sklearn (how does a dataset work according to the tutorial)***\n",
    "    - *The datasets are organized such that there is a training set, and the target that you would like for the model to be able to predict*\n",
    "- ***What is the dimensionality of the .data part of a dataset and what is the size of each dimension?***\n",
    "    - *There are 64 attributes to the .data part. The size is an integer from 0-16*\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-completion",
   "metadata": {},
   "source": [
    "***We won't do the whole tutorial. Try it out: I'd like you to work thorough up to and including the section Building a pipeline [tutorial](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-caribbean",
   "metadata": {},
   "source": [
    "**Tutorial setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "    categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-moscow",
   "metadata": {},
   "source": [
    "*We can find the target names*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-radiation",
   "metadata": {},
   "source": [
    "*We check the name of the datasets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-gasoline",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(twenty_train.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-tsunami",
   "metadata": {},
   "source": [
    "*We print the first lines of the first loaded file:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-visibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))\n",
    "print(twenty_train.target_names[twenty_train.target[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-partition",
   "metadata": {},
   "source": [
    "*For efficiency, the target set categories has been transformed into integers instead of strings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-penny",
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train.target[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-proof",
   "metadata": {},
   "source": [
    "*We can get the category names by the following method:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-vault",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in twenty_train.target[:10]:\n",
    "    print(twenty_train.target_names[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-bidder",
   "metadata": {},
   "source": [
    "**Extracting features from text files**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-watch",
   "metadata": {},
   "source": [
    "*Tokenizing text with `scikit-learn`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-robin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#initiliase count vector:\n",
    "count_vect = CountVectorizer()\n",
    "#make a fit with the count vector, with the training data\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "#Display the shape of the feature set (2257 rows and 35788 columns)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-framing",
   "metadata": {},
   "source": [
    "*With the use of vocubulary_.get() we can extract the relevant features, ergo: stuff that doesn't just exist few times. Hence our feature set has been reduced to 4690 instead of 35788*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-challenge",
   "metadata": {},
   "source": [
    "*In order to account for some documents being longer than others, and therefore creating a bias, the word count is made a term frequency, based on the length of the document*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-encyclopedia",
   "metadata": {},
   "source": [
    "**Training a classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-childhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-waterproof",
   "metadata": {},
   "source": [
    "**Building a pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-rendering",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-institute",
   "metadata": {},
   "source": [
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-bikini",
   "metadata": {},
   "source": [
    "***Exercise 2.2***\n",
    "\n",
    "*Did you do the work*\n",
    "\n",
    "- ***Describe in your own words the dataset used in the tutorial.***\n",
    "    - *The dataset is a set of posts from newsgroups. It consists of a training and test set. The target says what category a text belongs to, for instance `soc.religion.christian`.*\n",
    "- ***Investigate further: what kind of folder/file structure does the sklearn.datasets.load_files function expect?***\n",
    "    - *The function takes a folder where the different categories are its own subfolder.*\n",
    "- ***What is the \"bag-of-words\" representation of text? How does this strategy turn text into data of the kind described above?***\n",
    "    - *It takes every word that are in all the texts represented, and turns them into a feature set with all those words. One could make sure to exclude the words that occur few times, including 'and'...*\n",
    "- ***Once you've built the classifier, play around with it a bit. Describe the content of the predicted variable.***\n",
    "    - *The predicted variable takes a sentence or word and predicts which subject it belongs to, like computer science, religion etc.*\n",
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-reducing",
   "metadata": {},
   "source": [
    "## Part 3: KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-liberia",
   "metadata": {},
   "source": [
    "***Exercise 3.1***\n",
    "\n",
    "*How does K-nearest-neighbors work? Explain in your own words. Explain in your own words: What is the curse of dimensionality? Use figure 12-6 in DSFS as part of your explanation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-france",
   "metadata": {},
   "source": [
    "***Answer:*** *In figure 12-6, it can be seen that the more dimensions you add to a model for KNN, the greater the average distance is between each point - this is inevitable, since you add another dimension, then the summed distance has to be greater.*\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-punch",
   "metadata": {},
   "source": [
    "***We know from last week's exercises that the focus crimes PROSTITUTION, DRUG/NARCOTIC and DRIVING UNDER THE INFLUENCE tend to be concentrated in certain neighborhoods, so we focus on those crime types since they will make the most sense a KNN - map.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-choir",
   "metadata": {},
   "source": [
    "***Exercise 3.2***\n",
    "\n",
    "*Begin by using folium (see Week4) to plot all incidents of the three crime types on their own map. This will give you an idea of how the varioius crimes are distributed across the city.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-carolina",
   "metadata": {},
   "source": [
    "***Answer:***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-volunteer",
   "metadata": {},
   "source": [
    "*We start by importing folium and pandas*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-combat",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-portsmouth",
   "metadata": {},
   "source": [
    "*We import the policing dataframe and parse the date and time to a datetime column*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-astrology",
   "metadata": {},
   "outputs": [],
   "source": [
    "policing_dataframe = pd.read_csv('../rawdata/Police_Department_Incident_Reports__Historical_2003_to_May_2018.csv', parse_dates=[['Date', 'Time']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_p_df_prostitution = (policing_dataframe['Category'] >= \"PROSTITUTION\") & (policing_dataframe['Date_Time'] > \"2017-06-01\") & (policing_dataframe['Date_Time'] < \"2017-11-01\")\n",
    "mask_p_df_drug = (policing_dataframe['Category'] >= \"DRUG/NARCOTIC\") & (policing_dataframe['Date_Time'] > \"2017-06-01\") & (policing_dataframe['Date_Time'] < \"2017-11-01\")\n",
    "mask_p_df_dui = (policing_dataframe['Category'] >= \"DRIVING UNDER THE INFLUENCE\") & (policing_dataframe['Date_Time'] > \"2017-06-01\") & (policing_dataframe['Date_Time'] < \"2017-11-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-billion",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df_prostitution = policing_dataframe.loc[mask_p_df_prostitution].reset_index(drop=True)\n",
    "p_df_drug = policing_dataframe.loc[mask_p_df_drug].reset_index(drop=True)\n",
    "p_df_dui = policing_dataframe.loc[mask_p_df_dui].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-pregnancy",
   "metadata": {},
   "source": [
    "*We start by getting the lon and lat of the dataframes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-coast",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_y_prostitution = list(zip(list(p_df_prostitution[\"Y\"]), list(p_df_prostitution[\"X\"])))\n",
    "x_y_drug = list(zip(list(p_df_drug[\"Y\"]), list(p_df_drug[\"X\"])))\n",
    "x_y_dui = list(zip(list(p_df_dui[\"Y\"]), list(p_df_dui[\"X\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-contrary",
   "metadata": {},
   "source": [
    "*We first plot `PROSTITUTION`:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-variance",
   "metadata": {},
   "outputs": [],
   "source": [
    "SF_map1 = folium.Map([37.77919, -122.41914], zoom_start=13, tiles = \"Stamen Toner\")\n",
    "folium.Marker([37.77919, -122.41914], popup='SF City Hall').add_to(SF_map1)\n",
    "\n",
    "for x,y in x_y_prostitution:\n",
    "    folium.CircleMarker([x, y],\n",
    "                    radius=1,\n",
    "                    color='red',\n",
    "                    ).add_to(SF_map1)\n",
    "\n",
    "SF_map1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-uzbekistan",
   "metadata": {},
   "source": [
    "*We now plot `DRUG/NARCOTICS`:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "SF_map2 = folium.Map([37.77919, -122.41914], zoom_start=13, tiles = \"Stamen Toner\")\n",
    "folium.Marker([37.77919, -122.41914], popup='SF City Hall').add_to(SF_map2)\n",
    "\n",
    "for x,y in x_y_drug:\n",
    "    folium.CircleMarker([x, y],\n",
    "                    radius=1,\n",
    "                    color='blue',\n",
    "                    ).add_to(SF_map2)\n",
    "\n",
    "SF_map2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-turtle",
   "metadata": {},
   "source": [
    "*We now plot `DRIVING UNDER THE INFLUENCE`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-compiler",
   "metadata": {},
   "outputs": [],
   "source": [
    "SF_map3 = folium.Map([37.77919, -122.41914], zoom_start=13, tiles = \"Stamen Toner\")\n",
    "folium.Marker([37.77919, -122.41914], popup='SF City Hall').add_to(SF_map3)\n",
    "\n",
    "for x,y in x_y_dui:\n",
    "    folium.CircleMarker([x, y],\n",
    "                    radius=1,\n",
    "                    color='green',\n",
    "                    ).add_to(SF_map3)\n",
    "\n",
    "SF_map3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-logging",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-calibration",
   "metadata": {},
   "source": [
    "***Exercise 3.3***\n",
    "\n",
    "*Next, it's time to set up your model based on the actual data. I recommend that you try out sklearn's KNeighborsClassifier. For an intro, start with this tutorial and follow the link to get a sense of the usage.*\n",
    "\n",
    "- *You don't have to think a lot about testing/trainig and accuracy for this exercise. We're mostly interested in creating a map that's not too problematic. But do calculate the number of observations of each crime-type respectively. You'll find that the levels of each crime varies (lots of drug arrests, an intermediate amount of prostitiution registered, and very little drunk driving in the dataset). Since the algorithm classifies each point according to it's neighbors, what could a consequence of this imbalance in the number of examples from each class mean for your map?*\n",
    "- *You can make the dataset 'balanced' by grabbing an equal number of examples from each crime category.*\n",
    "    - *How do you expect that will change the KNN result?*\n",
    "    - *In which situations is the balanced map useful -*\n",
    "    - *When is the map where data is in proportion to occurrences useful?*\n",
    "    - *Choose which map you will work on in the following.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-occupation",
   "metadata": {},
   "source": [
    "***Answer:***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-symbol",
   "metadata": {},
   "source": [
    "*We calculate the number of observations for each crime type:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of observations for prostitution is:\", len(p_df_prostitution))\n",
    "print(\"The number of observations for drug/narcotic is:\", len(p_df_drug))\n",
    "print(\"The number of observations for driving under the influence is:\", len(p_df_dui))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-bernard",
   "metadata": {},
   "source": [
    "*What consequence could it have that there are more observations in one group than another?*\n",
    "- *This could lead to the model being more likely to predict the category that is the higher amount of cases, no matter where you are in the map (creating a bias)*\n",
    "- *We expect the result to be changed by balancing the datasets, creating less bias*\n",
    "- *The balanced map is useful in a situation, where you need to predict where a crime is most likely to happen*\n",
    "- *If you need to predict where crimes are more likely to happen, then the dataset that is in proportion is appropriate*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-quarterly",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-proposition",
   "metadata": {},
   "source": [
    "***Exercise 3.4***\n",
    "\n",
    "*Now create an approximately square grid of point that runs over SF. You get to decide the grid-size, but I recommend somewhere between $50\\times50$ and $100\\times100$ points. I recommend using folium for this task.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-attitude",
   "metadata": {},
   "source": [
    "***Answer:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-chance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-korean",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geojson_grid(upper_right, lower_left, n=6):\n",
    "    \"\"\"Returns a grid of geojson rectangles, and computes the exposure in each section of the grid based on the vessel data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    upper_right: array_like\n",
    "        The upper right hand corner of \"grid of grids\" (the default is the upper right hand [lat, lon] of the USA).\n",
    "\n",
    "    lower_left: array_like\n",
    "        The lower left hand corner of \"grid of grids\"  (the default is the lower left hand [lat, lon] of the USA).\n",
    "\n",
    "    n: integer\n",
    "        The number of rows/columns in the (n,n) grid.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    list\n",
    "        List of \"geojson style\" dictionary objects   \n",
    "    \"\"\"\n",
    "\n",
    "    all_boxes = []\n",
    "\n",
    "    lat_steps = np.linspace(lower_left[0], upper_right[0], n+1)\n",
    "    lon_steps = np.linspace(lower_left[1], upper_right[1], n+1)\n",
    "\n",
    "    lat_stride = lat_steps[1] - lat_steps[0]\n",
    "    lon_stride = lon_steps[1] - lon_steps[0]\n",
    "\n",
    "    for lat in lat_steps[:-1]:\n",
    "        for lon in lon_steps[:-1]:\n",
    "            # Define dimensions of box in grid\n",
    "            upper_left = [lon, lat + lat_stride]\n",
    "            upper_right = [lon + lon_stride, lat + lat_stride]\n",
    "            lower_right = [lon + lon_stride, lat]\n",
    "            lower_left = [lon, lat]\n",
    "\n",
    "            # Define json coordinates for polygon\n",
    "            coordinates = [\n",
    "                upper_left,\n",
    "                upper_right,\n",
    "                lower_right,\n",
    "                lower_left,\n",
    "                upper_left\n",
    "            ]\n",
    "\n",
    "            geo_json = {\"type\": \"FeatureCollection\",\n",
    "                        \"properties\":{\n",
    "                            \"lower_left\": lower_left,\n",
    "                            \"upper_right\": upper_right\n",
    "                        },\n",
    "                        \"features\":[]}\n",
    "\n",
    "            grid_feature = {\n",
    "                \"type\":\"Feature\",\n",
    "                \"geometry\":{\n",
    "                    \"type\":\"Polygon\",\n",
    "                    \"coordinates\": [coordinates],\n",
    "                }\n",
    "            }\n",
    "\n",
    "            geo_json[\"features\"].append(grid_feature)\n",
    "\n",
    "            all_boxes.append(geo_json)\n",
    "\n",
    "    return all_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-rhythm",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_left = [37.713995, -122.517391]\n",
    "upper_right = [37.814491, -122.352865]\n",
    "m = folium.Map(location=[37.77919, -122.41914], zoom_start=12, tiles = \"Stamen Toner\")\n",
    "grid = get_geojson_grid(upper_right, lower_left , n=50)\n",
    "\n",
    "for i, geo_json in enumerate(grid):\n",
    "\n",
    "    color = plt.cm.Reds(i / len(grid))\n",
    "    color = mpl.colors.to_hex(color)\n",
    "\n",
    "    gj = folium.GeoJson(geo_json,\n",
    "                        style_function=lambda feature, color=color: {\n",
    "                                                                        'fillColor': color,\n",
    "                                                                        'color':\"black\",\n",
    "                                                                        'weight': 1,\n",
    "                                                                        'fillOpacity': 0.55,\n",
    "                                                                    })\n",
    "    popup = folium.Popup(\"example popup {}\".format(i))\n",
    "    gj.add_child(popup)\n",
    "\n",
    "    m.add_child(gj)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-twins",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meters_to_degrees(meters):\n",
    "    degrees_0_0001_in_meters = 11.1\n",
    "    return round((meters / degrees_0_0001_in_meters) * 0.0001, 9)\n",
    "\n",
    "size_of_grid = 200\n",
    "degrees = meters_to_degrees(size_of_grid)\n",
    "to_bin = lambda x: np.floor(x / degrees) * degrees\n",
    "policing_dataframe[\"latbin\"] = policing_dataframe.Y.map(to_bin)\n",
    "policing_dataframe[\"lonbin\"] = policing_dataframe.X.map(to_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-judgment",
   "metadata": {},
   "outputs": [],
   "source": [
    "policing_dataframe[\"latbin\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-huntington",
   "metadata": {},
   "outputs": [],
   "source": [
    "SF_map4 = folium.Map([37.77919, -122.41914], zoom_start=13, tiles = \"Stamen Toner\")\n",
    "x_y_policing_dataframe = list(zip(list(policing_dataframe[\"latbin\"].unique()), list(policing_dataframe[\"lonbin\"].unique())))\n",
    "\n",
    "for x,y in x_y_policing_dataframe:\n",
    "    folium.CircleMarker([x, y],\n",
    "                    radius=1,\n",
    "                    color='green',\n",
    "                    ).add_to(SF_map4)\n",
    "\n",
    "SF_map4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-distance",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-addition",
   "metadata": {},
   "source": [
    "***Exercise 3.5***\n",
    "\n",
    "*Visualize your model by coloring the grid, coloring each grid point according to it's category. Create a plot of this kind for models where each point is colored according to the majority of its $5$, $10$, and $30$ nearest neighbors. Describe what happens to the map as you increase the number of neighbors, K*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-rescue",
   "metadata": {},
   "source": [
    "***Answer:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-bacteria",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "under-picnic",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-reality",
   "metadata": {},
   "source": [
    "***Exercise 3.6***\n",
    "\n",
    "*To see an example, click here. This one is a 100x100 grid based on crimes from 1st January 2017 until the end of 2018. And the categories are narcotics, prostitution and vehicle theft.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-tribute",
   "metadata": {},
   "source": [
    "***Answer:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-buyer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
